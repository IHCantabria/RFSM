{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from netCDF4 import Dataset, date2num\n",
    "import glob\n",
    "import scipy.io\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pomegranate import *\n",
    "\n",
    "from scipy.stats import genextreme as gev\n",
    "from scipy.stats import gumbel_l as gumbel\n",
    "from scipy.stats import gamma as gamma\n",
    "from scipy.stats import expon as exp\n",
    "from scipy.stats import lognorm as logn\n",
    "from scipy.stats import gumbel_r as gumbel_r\n",
    "from scipy.stats import powerlaw as powerlaw\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sma\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import warnings\n",
    "from pyproj import Proj, transform\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(); sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ajuste_GEV_KMA_Frechet(data, plot=False):\n",
    "    #Ajuste Gumbel\n",
    "    func=getattr(st,'genextreme')\n",
    "    paramt = func.fit(data,fc=0.0000000001)\n",
    "    nlogL = np.log(func.pdf(data,*paramt)).sum()\n",
    "    parmhatgev = func.fit(data)\n",
    "    nlogLGev = np.log(func.pdf(data,*parmhatgev)).sum()\n",
    "    \n",
    "    if -parmhatgev[0]>0:\n",
    "        isgev = np.sum((np.abs(nlogLGev)-np.abs(nlogL))>=1.92)\n",
    "        if  isgev==1:\n",
    "                paramGev= parmhatgev\n",
    "        else:\n",
    "             paramGev = paramt\n",
    "    else:\n",
    "        paramGev= parmhatgev\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax=plt.subplots(2, 2, figsize=(12, 8))\n",
    "        data.plot(kind='hist', bins=50, density =True, alpha=0.5, color='red',ax=ax[0,0])\n",
    "        dataYLim = ax[0,0].get_ylim()\n",
    "        distribution=getattr(st,'genextreme')\n",
    "        best_distribution = distribution\n",
    "        best_dist = getattr(st,'genextreme')\n",
    "        best_fit_params = paramGev\n",
    "        best_fit_name='genextreme'\n",
    "\n",
    "        #----------------------------------------------------------------------------------- \n",
    "        pdf = make_pdf(best_dist, best_fit_params)\n",
    "\n",
    "        pdf.plot(lw=2, label='PDF', legend=True, ax=ax[0,0])\n",
    "        data.plot(kind='hist', bins=50, density=True, alpha=0.5, label='Data', legend=True, ax=ax[0,0])\n",
    "\n",
    "        param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "        param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "        dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "\n",
    "        ax[0,0].set_title(u'Best probability density function \\n' + dist_str)\n",
    "        ax[0,0].set_xlabel(u'Data')\n",
    "        ax[0,0].set_ylabel('Frequency')\n",
    "        ax[0,0].set_ylim(dataYLim)\n",
    "        #-----------------------------------------------------------------------------------     \n",
    "        data=data.values.flatten()\n",
    "        PQmax_1 = best_dist.cdf(data.astype(float),*best_fit_params);\n",
    "        Qemp = np.sort(data);\n",
    "        kk = np.arange(1,len(data)+1);\n",
    "        prob = kk/(len(data));\n",
    "        Qlogn  = best_dist.ppf(prob,*best_fit_params);\n",
    "        ax[0,1].plot(Qemp,Qlogn,'.k')\n",
    "        ax[0,1].plot(np.arange(min(data),max(data)),np.arange(min(data),max(data)),'-b')\n",
    "        ax[0,1].set_title('Q-Q Plot \\n' + dist_str)\n",
    "        ax[0,1].set_xlabel(u'Empirical')\n",
    "        ax[0,1].set_ylabel('Model')\n",
    "        #-----------------------------------------------------------------------------------     \n",
    "        ecdf=ECDF(data)\n",
    "        x = np.linspace(min(data),max(data),200)\n",
    "        y = best_dist.cdf(x, *best_fit_params)\n",
    "        ax[1,0].plot(x, y)\n",
    "        ax[1,0].scatter(data.astype(float),ecdf(data))\n",
    "        ax[1,0].set_title('Best Cumulative density function \\n' + dist_str)\n",
    "        ax[1,0].set_xlabel(u'Data')\n",
    "        ax[1,0].set_ylabel('Prob')\n",
    "        #-----------------------------------------------------------------------------------    \n",
    "        x_r=np.arange(0.01,1000)\n",
    "        prob_t=(1-1/x_r)\n",
    "        y_r=best_dist.ppf(prob_t,*best_fit_params)\n",
    "        ax[1,1].plot(x_r,y_r,'-b')\n",
    "\n",
    "        ecdf=ECDF(data)\n",
    "        t_r=1/(1-ecdf(data))\n",
    "        ax[1,1].plot(t_r,data,'.k')\n",
    "        ax[1,1].set_xscale(\"log\")\n",
    "        ax[1,1].set_xlabel(u'Return Periods')\n",
    "        ax[1,1].set_ylabel('Return Values')\n",
    "        ax[1,1].set_title('Return Values Plot \\n' + dist_str)\n",
    "\n",
    "        print(best_fit_params)\n",
    "        fig.tight_layout()  \n",
    "    func=getattr(st,'genextreme')\n",
    "    \n",
    "    PeriodoRetorno=[2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100,200,500,1000]\n",
    "    TWL = pd.DataFrame(index=PeriodoRetorno,columns=['TWL'])\n",
    "    for i in PeriodoRetorno:\n",
    "        TWL.loc[i] = func.ppf((1-1/i),*paramGev)\n",
    "        \n",
    "    return TWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matDatenum2PYDatetime(datenumVec,unitTime = 'D'):\n",
    "    datetimeVec = pd.to_datetime(datenumVec-719529, unit=unitTime,errors='coerce')\n",
    "    datetimeNum = datenumVec-719529\n",
    "    return datetimeVec,datetimeNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcf_Dinamicas_DOW(path_files,path_output,model,scenario):\n",
    "    directory = list() \n",
    "    for file in os.listdir(path_files):\n",
    "        if file.startswith(\"Punto\"):\n",
    "            directory.append(file)\n",
    "    names_point = list()\n",
    "    lon = list()\n",
    "    lat = list()\n",
    "    for d, dd in enumerate(directory):\n",
    "        names_point.append(dd[:-19])\n",
    "        lon.append(float(dd[-17:-10]))\n",
    "        lat.append(float(dd[-8:-1]))\n",
    "        \n",
    "    for i,ii in enumerate(tqdm.tqdm(directory)):\n",
    "        if i ==0:\n",
    "            mat    = scipy.io.loadmat(path_files+directory[i]+'/'+directory[i]+'_'+model+'_'+scenario+'.mat')\n",
    "            time   = mat['time'].flatten()\n",
    "            hs_o   = mat['hs'].reshape(1,-1)\n",
    "            tps_o  = mat['tps'].reshape(1,-1)\n",
    "            dir_o  = mat['dir'].reshape(1,-1)\n",
    "            zeta_o = mat['zeta'].reshape(1,-1)\n",
    "            tm02_o = mat['tm02'].reshape(1,-1)\n",
    "        else:\n",
    "            mat = scipy.io.loadmat(path_files+directory[i]+'/'+directory[i]+'_'+model+'_'+scenario+'.mat')\n",
    "            hs_o   = np.concatenate((hs_o, mat['hs'].reshape(1,-1)), axis=0)\n",
    "            tps_o  = np.concatenate((tps_o, mat['tps'].reshape(1,-1)), axis=0)\n",
    "            dir_o  = np.concatenate((dir_o, mat['dir'].reshape(1,-1)), axis=0)\n",
    "            zeta_o = np.concatenate((zeta_o, mat['zeta'].reshape(1,-1)), axis=0)\n",
    "            tm02_o = np.concatenate((tm02_o, mat['tm02'].reshape(1,-1)), axis=0)\n",
    "        \n",
    "    time_py = matDatenum2PYDatetime(time,unitTime = 'D')[0]\n",
    "    \n",
    "    nc = Dataset(path_output+'PtosObjtvo_DIN_'+model+'_'+scenario+'.nc', 'w', format='NETCDF4')\n",
    "    # Global Attributes \n",
    "    nc.description= 'Contiene las dinámicas del modelo '+model+' en el escenario '+scenario  \n",
    "    # nc dimensions\n",
    "    nc.createDimension('lon',  len(lon))\n",
    "    nc.createDimension('lat',  len(lat))\n",
    "    nc.createDimension('time',len(time))\n",
    "    #nc.createDimension('point',len(names_point))\n",
    "    # crear variables\n",
    "    xx_nc=nc.createVariable('lon','float32', ('lon'))\n",
    "    yy_nc=nc.createVariable('lat','float32', ('lat'))\n",
    "    time_nc=nc.createVariable('time','float32',('time'))\n",
    "    #point_nc = nc.createVariable('point','int',('point'))\n",
    "    hs_nc=nc.createVariable('hs','float32', ('time','lon'))\n",
    "    tps_nc=nc.createVariable('tps','float32', ('time','lon'))\n",
    "    dir_nc=nc.createVariable('dir','float32', ('time','lon'))\n",
    "    zeta_nc=nc.createVariable('zeta','float32', ('time','lon'))\n",
    "    tm02_nc=nc.createVariable('tm02','float32', ('time','lon'))\n",
    "    #units\n",
    "    xx_nc.units = 'degrees_east'\n",
    "    yy_nc.units = 'degrees_north'\n",
    "    time_nc.units='days since '+str(time_py[0].year)+'-01-01'\n",
    "    hs_nc.units='m'\n",
    "    tps_nc.units='s'\n",
    "    dir_nc.units='º'\n",
    "    zeta_nc.units='º'\n",
    "    tm02_nc.units='s'\n",
    "    #long_name\n",
    "    xx_nc.long_name = 'longitude coordinate'\n",
    "    yy_nc.long_name = 'latitud coordinate'\n",
    "    time_nc.long_name='dias del año'\n",
    "    hs_nc.long_name='Altura de ola'\n",
    "    tps_nc.long_name='Período de pico suavizado'\n",
    "    dir_nc.long_name='Dirección'\n",
    "    zeta_nc.long_name='Marea meteorológica'\n",
    "    tm02_nc.long_name='Periodo medio'\n",
    "    # calendar\n",
    "    time_nc.calendar = 'standard'\n",
    "    # rellenar variables\n",
    "    #point_nc[:]=np.arange(1,len(names_point)+1)\n",
    "    xx_nc[:]=lon\n",
    "    yy_nc[:]=lat\n",
    "    time_nc[:]=date2num(time_py.to_pydatetime(), units='days since '+str(time_py[0].year)+'-01-01', calendar='standard')\n",
    "    hs_nc[:]=hs_o[:,:].T\n",
    "    tps_nc[:]=tps_o[:,:].T\n",
    "    dir_nc[:]=dir_o[:,:].T\n",
    "    zeta_nc[:] = zeta_o[:,:].T\n",
    "    tm02_nc[:]=tm02_o[:,:].T\n",
    "    nc.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcf_MareaAst(path_files,path_output):\n",
    "    directory = list() \n",
    "    for file in os.listdir(path_files):\n",
    "        if file.startswith(\"Punto\"):\n",
    "            directory.append(file)\n",
    "    names_point = list()\n",
    "    lon = list()\n",
    "    lat = list()\n",
    "    for d, dd in enumerate(directory):\n",
    "        names_point.append(dd[:-23])\n",
    "        lon.append(float(dd[-21:-14]))\n",
    "        lat.append(float(dd[-12:-5]))\n",
    "        \n",
    "    for i,ii in enumerate(tqdm.tqdm(directory)):\n",
    "        if i ==0:\n",
    "            mat    = scipy.io.loadmat(path_files+directory[i])\n",
    "            time   = mat['time'].flatten()\n",
    "            lon_tide_o = mat['lon_tide']\n",
    "            lat_tide_o = mat['lat_tide']\n",
    "            tide_o  = mat['tide'].reshape(1,-1)\n",
    "            u_o  = mat['u'].reshape(1,-1)\n",
    "            v_o  = mat['v'].reshape(1,-1)\n",
    "        else:\n",
    "            mat    = scipy.io.loadmat(path_files+directory[i])\n",
    "            tide_o  =  np.concatenate((tide_o,mat['tide'].reshape(1,-1)),axis=0)\n",
    "            lon_tide_o = np.concatenate((lon_tide_o,mat['lon_tide']),axis=0)\n",
    "            lat_tide_o = np.concatenate((lat_tide_o,mat['lat_tide']),axis=0)\n",
    "            u_o  = np.concatenate((u_o,mat['u'].reshape(1,-1)),axis=0)\n",
    "            v_o  = np.concatenate((v_o,mat['v'].reshape(1,-1)),axis=0)\n",
    "        \n",
    "    time_py = matDatenum2PYDatetime(time,unitTime = 'D')[0]\n",
    "    nc = Dataset(path_output+'PtosObjtvo_MAT.nc', 'w', format='NETCDF4')\n",
    "    # Global Attributes \n",
    "    nc.description= 'Contiene la variable Marea Astronómica' \n",
    "    # nc dimensions\n",
    "    nc.createDimension('lon',  len(lon))\n",
    "    nc.createDimension('lat',  len(lat))\n",
    "    nc.createDimension('time',len(time))\n",
    "    #nc.createDimension('point',len(names_point))\n",
    "    # crear variables\n",
    "    xx_nc=nc.createVariable('lon','float32', ('lon'))\n",
    "    yy_nc=nc.createVariable('lat','float32', ('lat'))\n",
    "    time_nc=nc.createVariable('time','float32',('time'))\n",
    "    lon_tide_nc = nc.createVariable('lon_tide','float32', ('lon'))\n",
    "    lat_tide_nc = nc.createVariable('lat_tide','float32', ('lat'))\n",
    "    #point_nc = nc.createVariable('point','int',('point'))\n",
    "    tide_nc=nc.createVariable('tide','float32', ('time','lon'))\n",
    "    u_nc=nc.createVariable('u','float32', ('time','lon'))\n",
    "    v_nc=nc.createVariable('v','float32', ('time','lon'))\n",
    "    #units\n",
    "    xx_nc.units = 'degrees_east'\n",
    "    yy_nc.units = 'degrees_north'\n",
    "    time_nc.units='days since '+str(time_py[0].year)+'-01-01'\n",
    "    lon_tide_nc.units = 'degrees_east'\n",
    "    lat_tide_nc.units = 'degrees_north'\n",
    "    tide_nc.units='m'\n",
    "    u_nc.units='m'\n",
    "    v_nc.units='m'\n",
    "    #long_name\n",
    "    xx_nc.long_name = 'longitude coordinate'\n",
    "    yy_nc.long_name = 'latitud coordinate'\n",
    "    lon_tide_nc.long_name = 'longitude coordinate tide'\n",
    "    lat_tide_nc.long_name = 'latitud coordinate tide'\n",
    "    time_nc.long_name='dias del año'\n",
    "    tide_nc.long_name='Altura'\n",
    "    u_nc.long_name='Dirección horizontal'\n",
    "    v_nc.long_name='Dirección vertical'\n",
    "    \n",
    "    # calendar\n",
    "    time_nc.calendar = 'standard'\n",
    "    # rellenar variables\n",
    "    #point_nc[:]=np.arange(1,len(names_point)+1)\n",
    "    xx_nc[:]=lon\n",
    "    yy_nc[:]=lat\n",
    "    lon_tide_nc[:] = lon_tide_o\n",
    "    lat_tide_nc[:] = lat_tide_o\n",
    "    time_nc[:]=date2num(time_py.to_pydatetime(), units='days since '+str(time_py[0].year)+'-01-01', calendar='standard')\n",
    "    tide_nc[:]=tide_o[:,:].T\n",
    "    u_nc[:]=u_o[:,:].T\n",
    "    v_nc[:]=v_o[:,:].T\n",
    "    nc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcf_Dinamicas_SLR(path_files,path_output):\n",
    "    directory = list() \n",
    "    for file in os.listdir(path_files):\n",
    "        if file.startswith(\"Punto\"):\n",
    "            directory.append(file)\n",
    "    names_point = list()\n",
    "    lon = list()\n",
    "    lat = list()\n",
    "    for d, dd in enumerate(directory):\n",
    "        names_point.append(dd[:-27])\n",
    "        lon.append(float(dd[-25:-18]))\n",
    "        lat.append(float(dd[-16:-9]))\n",
    "        \n",
    "    for i,ii in enumerate(tqdm.tqdm(directory)):\n",
    "        if i ==0:\n",
    "            mat    = scipy.io.loadmat(path_files+directory[i])\n",
    "            time   = mat['time'].flatten()\n",
    "            m_ensemble45_o  = mat['m_ensemble45'].reshape(1,-1)\n",
    "            m_ensemble85_o  = mat['m_ensemble85'].reshape(1,-1)\n",
    "            P5_45_o  = mat['P5_45'].reshape(1,-1)\n",
    "            P5_85_o  = mat['P5_85'].reshape(1,-1)\n",
    "            P95_45_o = mat['P95_45'].reshape(1,-1)\n",
    "            P95_85_o = mat['P95_85'].reshape(1,-1)\n",
    "        else:\n",
    "            mat = scipy.io.loadmat(path_files+directory[i])\n",
    "            m_ensemble45_o   = np.concatenate((m_ensemble45_o, mat['m_ensemble45'].reshape(1,-1)), axis=0)\n",
    "            m_ensemble85_o  = np.concatenate((m_ensemble85_o, mat['m_ensemble85'].reshape(1,-1)), axis=0)\n",
    "            P5_45_o  = np.concatenate((P5_45_o, mat['P5_45'].reshape(1,-1)), axis=0)\n",
    "            P5_85_o  = np.concatenate((P5_85_o, mat['P5_85'].reshape(1,-1)), axis=0)\n",
    "            P95_45_o = np.concatenate((P95_45_o, mat['P95_45'].reshape(1,-1)), axis=0)\n",
    "            P95_85_o = np.concatenate((P95_85_o, mat['P95_85'].reshape(1,-1)), axis=0)\n",
    "        \n",
    "    time_py = matDatenum2PYDatetime(time,unitTime = 'D')[0]\n",
    "    \n",
    "    nc = Dataset(path_output+'PtosObjtvo_MSL.nc', 'w', format='NETCDF4')\n",
    "    # Global Attributes \n",
    "    nc.description= 'Contiene la variable SLR' \n",
    "    # nc dimensions\n",
    "    nc.createDimension('lon',  len(lon))\n",
    "    nc.createDimension('lat',  len(lat))\n",
    "    nc.createDimension('time',len(time))\n",
    "    #nc.createDimension('point',len(names_point))\n",
    "    # crear variables\n",
    "    xx_nc=nc.createVariable('lon','float32', ('lon'))\n",
    "    yy_nc=nc.createVariable('lat','float32', ('lat'))\n",
    "    time_nc=nc.createVariable('time','float32',('time'))\n",
    "    #point_nc = nc.createVariable('point','int',('point'))\n",
    "    m_ensemble45_nc=nc.createVariable('m_ensemble45','float32', ('time','lon'))\n",
    "    m_ensemble85_nc=nc.createVariable('m_ensemble85','float32', ('time','lon'))\n",
    "    P5_45_nc=nc.createVariable('P5_45','float32', ('time','lon'))\n",
    "    P5_85_nc=nc.createVariable('P5_85','float32', ('time','lon'))\n",
    "    P95_45_nc=nc.createVariable('P95_45','float32', ('time','lon'))\n",
    "    P95_85_nc=nc.createVariable('P95_85','float32', ('time','lon'))\n",
    "    #units\n",
    "    xx_nc.units = 'degrees_east'\n",
    "    yy_nc.units = 'degrees_north'\n",
    "    time_nc.units='days since '+str(time_py[0].year)+'-01-01'\n",
    "    m_ensemble45_nc.units='m'\n",
    "    m_ensemble85_nc.units='s'\n",
    "    P5_45_nc.units='º'\n",
    "    P5_85_nc.units='º'\n",
    "    P95_45_nc.units='º'\n",
    "    P95_85_nc.units='l'\n",
    "    #long_name\n",
    "    xx_nc.long_name = 'longitude coordinate'\n",
    "    yy_nc.long_name = 'latitud coordinate'\n",
    "    time_nc.long_name='dias del año'\n",
    "    m_ensemble45_nc.long_name='Ensemble RCP 45'\n",
    "    m_ensemble85_nc.long_name='Ensemble RCP 85'\n",
    "    P5_45_nc.long_name='Percentil 5% para el RCP 45'\n",
    "    P5_85_nc.long_name='Percentil 5% para el RCP 85'\n",
    "    P95_45_nc.long_name='Percentil 95% para el RCP 45'\n",
    "    P95_85_nc.long_name='Percentil 95% para el RCP 85'\n",
    "    # calendar\n",
    "    time_nc.calendar = 'standard'\n",
    "    # rellenar variables\n",
    "    #point_nc[:]=np.arange(1,len(names_point)+1)\n",
    "    xx_nc[:]=lon\n",
    "    yy_nc[:]=lat\n",
    "    time_nc[:]=date2num(time_py.to_pydatetime(), units='days since '+str(time_py[0].year)+'-01-01', calendar='standard')\n",
    "    m_ensemble45_nc[:]=m_ensemble45_o[:,:].T\n",
    "    m_ensemble85_nc[:]=m_ensemble85_o[:,:].T\n",
    "    P5_45_nc[:]=P5_45_o[:,:].T\n",
    "    P5_85_nc[:]=P5_85_o[:,:].T\n",
    "    P95_45_nc[:] = P95_45_o[:,:].T\n",
    "    P95_85_nc[:]=P95_85_o[:,:].T\n",
    "    nc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_fun (Tp,Hs,idplaya,slope,CStock):\n",
    "            L0=9.81/(2.*np.pi)*(Tp**2);\n",
    "            if idplaya == 1:\n",
    "                beta = slope; #% 1/30;\n",
    "                Irb=beta/np.sqrt(Hs/L0);#% Batjes,1974\n",
    "                # Stockdon,H.F. 2006; Runup2%\n",
    "                Stockdon_setup=CStock*beta*np.sqrt(Hs*L0);\n",
    "                #        s = find(Irb < 0.3);\n",
    "                #         Stockdon_setup(s)=0.016.*sqrt(Hs(s).*L0(s));\n",
    "                #Stockdon_setup=0.04.*sqrt(Hs.*L0);\n",
    "            else:\n",
    "                Stockdon_setup=0.08*np.sqrt(Hs*L0);\n",
    "            return Stockdon_setup\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files_extrems_CC(path_inputs_dinamicas,model,path_project,table_point_carac,refNMMA,fin_year_period,cod_epsg):\n",
    "    \"\"\" La siguiente función nos permite generar los ficheros de TWL para cada escenario de cambio climático.\n",
    "        A continuación se explican cada uno de los parámetros de entrada necesarios y los outputs generados\n",
    "        \n",
    "        Parámetros:\n",
    "        ---------------------\n",
    "        path_inputs_dinamicas   : string. path donde se encuentran los netcdf de las dinámicas para cada uno de los puntos de estudio\n",
    "        model                   : string. Modelo de cambio climático que se desea analizar\n",
    "        path_project            : string. path donde se encuentra el proyecto de RFSM\n",
    "        table_point_carac       : pandas dataframe. Dataframe con las coordenadas y características de los puntos donde se van a asignar las dinámicas\n",
    "        refNMMA                 : float. Referencia nivel medio del mar\n",
    "        fin_year_period         : array. Fecha de fin de cada uno de los períodos que se desean analizar\n",
    "        cod_epsg                : int. Código epsg del sistema de coordenadas utilizado en las coordendas de los puntos de estudio\n",
    "        \n",
    "        Salidas:\n",
    "        ---------------------\n",
    "        Ficheros csv para cada uno de modelos, escenarios y periodos.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    PtosObjtvo_DOW_his_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_DIN_'+model+'_his.nc')\n",
    "    PtosObjtvo_DOW_his_nc = PtosObjtvo_DOW_his_nc.sortby('lon')\n",
    "    PtosObjtvo_DOW_rcp45_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_DIN_'+model+'_rcp45.nc')\n",
    "    PtosObjtvo_DOW_rcp45_nc = PtosObjtvo_DOW_rcp45_nc.sortby('lon')\n",
    "    PtosObjtvo_DOW_rcp85_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_DIN_'+model+'_rcp85.nc')\n",
    "    PtosObjtvo_DOW_rcp85_nc = PtosObjtvo_DOW_rcp85_nc.sortby('lon')\n",
    "    PtosObjtvo_MSL_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_MSL.nc')\n",
    "    PtosObjtvo_MSL_nc = PtosObjtvo_MSL_nc.sortby('lon')\n",
    "    PtosObjtvo_MAT_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_MAT.nc')\n",
    "    PtosObjtvo_MAT_nc = PtosObjtvo_MAT_nc.sortby('lon')\n",
    "\n",
    "    PeriodoRetorno=[2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100,200,500,1000]\n",
    "    string_PR = [str(int) for int in PeriodoRetorno]\n",
    "    if os.path.exists(path_project+'sites/CC/')==False:\n",
    "        os.mkdir(path_project+'sites/CC/')\n",
    "    \n",
    "    for t in range(len(fin_year_period)):\n",
    "        print('Calculando TWL del período hasta '+str(fin_year_period[t]))\n",
    "        TWL_extrem_rcp_45_tab = pd.DataFrame(index=table_point_carac.index,columns=string_PR+['MSLR','SLR_5','SLR_95'])\n",
    "        TWL_extrem_rcp_85_tab = pd.DataFrame(index=table_point_carac.index,columns=string_PR+['MSLR','SLR_5','SLR_95'])\n",
    "        \n",
    "        TWL_extrem_hist_tab = pd.DataFrame(index=table_point_carac.index,columns=string_PR+['MSLR','SLR_5','SLR_95'])\n",
    "        TWL_extrem_hist_tab.loc[:,['MSLR','SLR_5','SLR_95']] = 0\n",
    "   \n",
    "        for i in tqdm.tqdm(range(len(table_point_carac))):\n",
    "\n",
    "            x_0 = table_point_carac.iloc[i,0]\n",
    "            y_0 = table_point_carac.iloc[i,1]\n",
    "\n",
    "            inProj = Proj(init='epsg:'+str(cod_epsg))\n",
    "            outProj = Proj(init='epsg:4326')\n",
    "\n",
    "            x,y = transform(inProj,outProj,x_0,y_0)\n",
    "\n",
    "\n",
    "            idoleaje = table_point_carac.iloc[i,2]\n",
    "            idmarea = table_point_carac.iloc[i,3]\n",
    "            idplaya = table_point_carac.iloc[i,3]\n",
    "            slope = table_point_carac.iloc[i,4]\n",
    "            CStock = table_point_carac.iloc[i,5]\n",
    "\n",
    "            point_selec_his = PtosObjtvo_DOW_his_nc.point.data[np.sqrt((x-PtosObjtvo_DOW_his_nc.lon.data)**2+(y-PtosObjtvo_DOW_his_nc.lat.data)**2).argmin()]\n",
    "            \n",
    "            point_selec_rcp_45 = PtosObjtvo_DOW_rcp45_nc.point.data[np.sqrt((x-PtosObjtvo_DOW_rcp45_nc.lon.data)**2+(y-PtosObjtvo_DOW_rcp45_nc.lat.data)**2).argmin()]\n",
    "            \n",
    "            point_selec_rcp_85 = PtosObjtvo_DOW_rcp85_nc.point.data[np.sqrt((x-PtosObjtvo_DOW_rcp85_nc.lon.data)**2+(y-PtosObjtvo_DOW_rcp85_nc.lat.data)**2).argmin()]\n",
    "            \n",
    "            point_selec_MSL = PtosObjtvo_MSL_nc.point.data[np.sqrt((x-PtosObjtvo_MSL_nc.lon.data)**2+(y-PtosObjtvo_MSL_nc.lat.data)**2).argmin()]\n",
    "            \n",
    "            point_selec_MAT = PtosObjtvo_MAT_nc.point.data[np.sqrt((x-PtosObjtvo_MAT_nc.lon.data)**2+(y-PtosObjtvo_MAT_nc.lat.data)**2).argmin()]\n",
    "            \n",
    "            \n",
    "            \n",
    "            Hs_his = PtosObjtvo_DOW_his_nc.sel(point = point_selec_his)['hs'].to_dataframe()\n",
    "            Hs_his.index = Hs_his.index.round('H')\n",
    "            try:\n",
    "                Hs_his[Hs_his.hs<0]=0.001\n",
    "            except:\n",
    "                Hs_his = Hs_his\n",
    "\n",
    "            Tp_his = PtosObjtvo_DOW_his_nc.sel(point = point_selec_his)['tps'].to_dataframe()\n",
    "            Tp_his.index = Tp_his.index.round('H')\n",
    "            try:\n",
    "                Tp_his[Tp_his.tps<=0]=np.mean(Tp_his[Tp_his>0],axis=0)\n",
    "            except:\n",
    "                Tp_his = Tp_his\n",
    "\n",
    "            SS_his = PtosObjtvo_DOW_his_nc.sel(point = point_selec_his)['zeta'].to_dataframe()\n",
    "            SS_his.index = SS_his.index.round('H')\n",
    "\n",
    "            Hs_rcp_45 = PtosObjtvo_DOW_rcp45_nc.sel(point = point_selec_rcp_45)['hs'].to_dataframe()\n",
    "            Hs_rcp_45.index = Hs_rcp_45.index.round('H')\n",
    "            try:\n",
    "                Hs_rcp_45[Hs_rcp_45.hs<0]=0.001\n",
    "            except:\n",
    "                Hs_rcp_45 = Hs_rcp_45\n",
    "\n",
    "            Tp_rcp_45 = PtosObjtvo_DOW_rcp45_nc.sel(point = point_selec_rcp_45)['tps'].to_dataframe()\n",
    "            Tp_rcp_45.index = Tp_rcp_45.index.round('H')\n",
    "            try:\n",
    "                Tp_rcp_45[Tp_rcp_45.tps<=0]=np.mean(Tp_rcp_45[Tp_rcp_45>0],axis=0)\n",
    "            except:\n",
    "                Tp_rcp_45 = Tp_rcp_45\n",
    "\n",
    "            SS_rcp45 = PtosObjtvo_DOW_rcp45_nc.sel(point = point_selec_rcp_45)['zeta'].to_dataframe()\n",
    "            SS_rcp45.index = SS_rcp45.index.round('H')\n",
    "\n",
    "            Hs_rcp_85 = PtosObjtvo_DOW_rcp85_nc.sel(point = point_selec_rcp_85)['hs'].to_dataframe()\n",
    "            Hs_rcp_85.index = Hs_rcp_85.index.round('H')\n",
    "            try:\n",
    "                Hs_rcp_85[Hs_rcp_85.hs<0]=0.001\n",
    "            except:\n",
    "                Hs_rcp_85 = Hs_rcp_85\n",
    "\n",
    "            Tp_rcp_85 = PtosObjtvo_DOW_rcp85_nc.sel(point = point_selec_rcp_85)['tps'].to_dataframe()\n",
    "            Tp_rcp_85.index = Tp_rcp_85.index.round('H')\n",
    "            try:\n",
    "                Tp_rcp_85[Tp_rcp_85.tps<=0]=np.mean(Tp_rcp_85[Tp_rcp_85>0],axis=0)\n",
    "            except:\n",
    "                Tp_rcp_85 = Tp_rcp_85\n",
    "\n",
    "            SS_rcp85 = PtosObjtvo_DOW_rcp85_nc.sel(point = point_selec_rcp_85)['zeta'].to_dataframe()\n",
    "            SS_rcp85.index = SS_rcp85.index.round('H')\n",
    "\n",
    "\n",
    "            MSLR_rcp45 = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['m_ensemble45'].to_dataframe()\n",
    "            MSLR_rcp45.index = MSLR_rcp45.index.round('H')\n",
    "\n",
    "            SLR_95_rcp45   = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['P95_45'].to_dataframe()\n",
    "            SLR_95_rcp45.index = SLR_95_rcp45.index.round('H')\n",
    "\n",
    "            SLR_5_rcp45   = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['P5_45'].to_dataframe()\n",
    "            SLR_5_rcp45.index = SLR_5_rcp45.index.round('H')\n",
    "\n",
    "\n",
    "            MSLR_rcp85 = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['m_ensemble85'].to_dataframe()\n",
    "            MSLR_rcp85.index = MSLR_rcp85.index.round('H')\n",
    "\n",
    "            SLR_95_rcp85   = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['P95_85'].to_dataframe()\n",
    "            SLR_95_rcp85.index = SLR_95_rcp85.index.round('H')\n",
    "\n",
    "            SLR_5_rcp85   = PtosObjtvo_MSL_nc.sel(point = point_selec_MSL)['P5_85'].to_dataframe()\n",
    "            SLR_5_rcp85.index = SLR_5_rcp85.index.round('H')\n",
    "\n",
    "            AT = PtosObjtvo_MAT_nc.sel(point = point_selec_MAT)['tide'].to_dataframe()\n",
    "            AT.index = AT.index.round('H')\n",
    "\n",
    "            PMVE =AT.iloc[AT.values.argmax()-3:AT.values.argmax()+4]\n",
    "\n",
    "            setup_hist  = pd.DataFrame(index=Tp_his.index, columns=['setup'])\n",
    "            setup_rcp45 = pd.DataFrame(index=Tp_rcp_45.index, columns=['setup'])\n",
    "            setup_rcp85 = pd.DataFrame(index=Tp_rcp_85.index, columns=['setup'])\n",
    "\n",
    "            setup_hist.iloc[:,0]  = setup_fun (Tp_his.values,Hs_his.values*idoleaje,idplaya,slope,CStock)\n",
    "            setup_rcp45.iloc[:,0] = setup_fun (Tp_rcp_45.values,Hs_rcp_45.values*idoleaje,idplaya,slope,CStock)\n",
    "            setup_rcp85.iloc[:,0] = setup_fun (Tp_rcp_85.values,Hs_rcp_45.values*idoleaje,idplaya,slope,CStock)\n",
    "\n",
    "            ## Cálculo de la cota de inundación / TWL\n",
    "\n",
    "            Result_hist  = pd.concat((setup_hist,SS_his,AT),axis = 1).dropna()\n",
    "            Result_rcp45 = pd.concat((setup_rcp45,SS_rcp45,AT),axis=1).dropna()\n",
    "            Result_rcp85 = pd.concat((setup_rcp85,SS_rcp85,AT),axis=1).dropna()\n",
    "\n",
    "            TWL_hist  = Result_hist.iloc[:,0]+Result_hist.iloc[:,1]*idmarea+Result_hist.iloc[:,2]*idmarea+refNMMA\n",
    "            TWL_rcp45 = Result_rcp45.iloc[:,0]+Result_rcp45.iloc[:,1]*idmarea+Result_rcp45.iloc[:,2]*idmarea+refNMMA\n",
    "            TWL_rcp85 = Result_rcp85.iloc[:,0]+Result_rcp85.iloc[:,1]*idmarea+Result_rcp85.iloc[:,2]*idmarea+refNMMA\n",
    "\n",
    "            \n",
    "            TWL_rcp_45 = pd.concat((TWL_hist,TWL_rcp45.loc[:'2040'])).resample('A').max().dropna().astype(float)\n",
    "            TWL_rcp_85 = pd.concat((TWL_hist,TWL_rcp85.loc[:'2040'])).resample('A').max().dropna().astype(float)\n",
    "            \n",
    "            TWL_extrem_rcp_45 = Ajuste_GEV_KMA_Frechet(TWL_rcp_45,plot=False)\n",
    "            TWL_extrem_rcp_85 = Ajuste_GEV_KMA_Frechet(TWL_rcp_85,plot=False)\n",
    "            \n",
    "            MSLR_RCP45   = MSLR_rcp45.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            MSLR_RCP85   = MSLR_rcp85.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            SLR_5_RCP45  = SLR_5_rcp45.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            SLR_5_RCP85  = SLR_5_rcp85.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            SLR_95_RCP45 = SLR_95_rcp45.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            SLR_95_RCP85 = SLR_95_rcp85.loc[:str(fin_year_period[t])].values[-1][0]\n",
    "            \n",
    "            TWL_extrem_rcp_45_tab.iloc[i,:len(PeriodoRetorno)]  = (TWL_extrem_rcp_45.values).reshape(1,-1)\n",
    "            TWL_extrem_rcp_85_tab.iloc[i,:len(PeriodoRetorno)]  = (TWL_extrem_rcp_85.values).reshape(1,-1)\n",
    "            \n",
    "            \n",
    "            TWL_extrem_rcp_45_tab.iloc[i,len(PeriodoRetorno):]  = [MSLR_RCP45, SLR_5_RCP45, SLR_95_RCP45]\n",
    "            TWL_extrem_rcp_85_tab.iloc[i,len(PeriodoRetorno):]  = [MSLR_RCP85, SLR_5_RCP85, SLR_95_RCP85]\n",
    "            \n",
    "        \n",
    "            TWL_extrem_his       = Ajuste_GEV_KMA_Frechet(TWL_hist.resample('A').max().dropna().astype(float),plot=False)\n",
    "            TWL_extrem_hist_tab.iloc[i,:len(PeriodoRetorno)] = (TWL_extrem_his.values).reshape(1,-1)\n",
    "            \n",
    "        TWL_extrem_rcp_45_tab.to_csv(path_project+'sites/CC/'+'TWL_extrem_'+model+'_RCP45_'+str(fin_year_period[t])+'.csv')\n",
    "        TWL_extrem_rcp_85_tab.to_csv(path_project+'sites/CC/'+'TWL_extrem_'+model+'_RCP85_'+str(fin_year_period[t])+'.csv')\n",
    "        TWL_extrem_hist_tab.to_csv(path_project+'sites/CC/'+'TWL_extrem_'+model+'_his.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files_extrems(path_inputs_dinamicas,model,path_project,table_point_carac,refNMMA,fin_year_period,cod_epsg):\n",
    "    \"\"\" La siguiente función nos permite generar los ficheros de TWL para cada escenario de cambio climático.\n",
    "        A continuación se explican cada uno de los parámetros de entrada necesarios y los outputs generados\n",
    "        \n",
    "        Parámetros:\n",
    "        ---------------------\n",
    "        path_inputs_dinamicas   : string. path donde se encuentran los netcdf de las dinámicas para cada uno de los puntos de estudio\n",
    "        model                   : string. Modelo de cambio climático que se desea analizar\n",
    "        path_project            : string. path donde se encuentra el proyecto de RFSM\n",
    "        table_point_carac       : pandas dataframe. Dataframe con las coordenadas y características de los puntos donde se van a asignar las dinámicas\n",
    "        refNMMA                 : float. Referencia nivel medio del mar\n",
    "        fin_year_period         : array. Fecha de fin de cada uno de los períodos que se desean analizar\n",
    "        cod_epsg                : int. Código epsg del sistema de coordenadas utilizado en las coordendas de los puntos de estudio\n",
    "        \n",
    "        Salidas:\n",
    "        ---------------------\n",
    "        Ficheros csv para cada uno de modelos, escenarios y periodos.\n",
    "    \n",
    "    \"\"\"\n",
    "    PtosObjtvo_ROW_his_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_DIN_ROW_his.nc')\n",
    "    PtosObjtvo_MAT_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_MAT.nc')\n",
    "\n",
    "    PeriodoRetorno=[2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100,200,500,1000]\n",
    "    string_PR = [str(int) for int in PeriodoRetorno]\n",
    "    \n",
    "    print('Calculando TWL del período hasta '+str(fin_year_period[t]))\n",
    "\n",
    "    TWL_extrem_hist_tab = pd.DataFrame(index=table_point_carac.index,columns=string_PR+['MSLR','SLR_5','SLR_95'])\n",
    "    TWL_extrem_hist_tab.loc[:,['MSLR','SLR_5','SLR_95']] = 0\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(table_point_carac))):\n",
    "\n",
    "        x_0 = table_point_carac.iloc[i,0]\n",
    "        y_0 = table_point_carac.iloc[i,1]\n",
    "\n",
    "        inProj = Proj(init='epsg:'+str(cod_epsg))\n",
    "        outProj = Proj(init='epsg:4326')\n",
    "\n",
    "        x,y = transform(inProj,outProj,x_0,y_0)\n",
    "\n",
    "\n",
    "        idoleaje = table_point_carac.iloc[i,2]\n",
    "        idmarea = table_point_carac.iloc[i,3]\n",
    "        idplaya = table_point_carac.iloc[i,3]\n",
    "        slope = table_point_carac.iloc[i,4]\n",
    "        CStock = table_point_carac.iloc[i,5]\n",
    "\n",
    "        point_selec_his = PtosObjtvo_ROW_his_nc.point.data[np.sqrt((x-PtosObjtvo_ROW_his_nc.lon.data)**2+(y-PtosObjtvo_ROW_his_nc.lat.data)**2).argmin()]\n",
    "\n",
    "        point_selec_MAT = PtosObjtvo_MAT_nc.point.data[np.sqrt((x-PtosObjtvo_MAT_nc.lon.data)**2+(y-PtosObjtvo_MAT_nc.lat.data)**2).argmin()]\n",
    "\n",
    "\n",
    "\n",
    "        Hs_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his)['hs'].to_dataframe()\n",
    "        Hs_his.index = Hs_his.index.round('H')\n",
    "        try:\n",
    "            Hs_his[Hs_his.hs<0]=0.001\n",
    "        except:\n",
    "            Hs_his = Hs_his\n",
    "\n",
    "        Tp_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his)['tps'].to_dataframe()\n",
    "        Tp_his.index = Tp_his.index.round('H')\n",
    "        try:\n",
    "            Tp_his[Tp_his.tps<=0]=np.mean(Tp_his[Tp_his>0],axis=0)\n",
    "        except:\n",
    "            Tp_his = Tp_his\n",
    "\n",
    "        SS_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his)['zeta'].to_dataframe()\n",
    "        SS_his.index = SS_his.index.round('H')\n",
    "\n",
    "\n",
    "        AT = PtosObjtvo_MAT_nc.sel(point = point_selec_MAT)['tide'].to_dataframe()\n",
    "        AT.index = AT.index.round('H')\n",
    "\n",
    "        PMVE =AT.iloc[AT.values.argmax()-3:AT.values.argmax()+4]\n",
    "\n",
    "        setup_hist  = pd.DataFrame(index=Tp_his.index, columns=['setup'])\n",
    "\n",
    "        setup_hist.iloc[:,0]  = setup_fun (Tp_his.values,Hs_his.values*idoleaje,idplaya,slope,CStock)\n",
    "\n",
    "        ## Cálculo de la cota de inundación / TWL\n",
    "\n",
    "        Result_hist  = pd.concat((setup_hist,SS_his,AT),axis = 1).dropna()\n",
    "\n",
    "        TWL_hist  = Result_hist.iloc[:,0]+Result_hist.iloc[:,1]*idmarea+Result_hist.iloc[:,2]*idmarea+refNMMA\n",
    "\n",
    "        TWL_extrem_his       = Ajuste_GEV_KMA_Frechet(TWL_hist.resample('A').max().dropna().astype(float),plot=False)\n",
    "        TWL_extrem_hist_tab.iloc[i,:len(PeriodoRetorno)] = (TWL_extrem_his.values).reshape(1,-1)\n",
    "            \n",
    "        TWL_extrem_hist_tab.to_csv(path_project+'sites/'+'TWL_extrem_'+model+'_his.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files_extrems(path_inputs_dinamicas,path_project,table_point_carac,refNMMA,cod_epsg):\n",
    "    \"\"\" La siguiente función nos permite generar los ficheros de TWL para el escenario ROW.\n",
    "        A continuación se explican cada uno de los parámetros de entrada necesarios y los outputs generados\n",
    "        \n",
    "        Parámetros:\n",
    "        ---------------------\n",
    "        path_inputs_dinamicas   : string. path donde se encuentran los netcdf de las dinámicas para cada uno de los puntos de estudio\n",
    "        path_project            : string. path donde se encuentra el proyecto de RFSM\n",
    "        table_point_carac       : pandas dataframe. Dataframe con las coordenadas y características de los puntos donde se van a asignar las dinámicas\n",
    "        refNMMA                 : float. Referencia nivel medio del mar\n",
    "        cod_epsg                : int. Código epsg del sistema de coordenadas utilizado en las coordendas de los puntos de estudio    \n",
    "        \n",
    "        Salidas:\n",
    "        ---------------------\n",
    "        Ficheros csv para cada uno de modelos, escenarios y periodos.\n",
    "    \n",
    "    \"\"\"\n",
    "    PtosObjtvo_ROW_his_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_DIN_ROW_his.nc')\n",
    "    PtosObjtvo_MAT_nc = xr.open_dataset(path_inputs_dinamicas+'PtosObjtvo_MAT.nc')\n",
    "\n",
    "    PeriodoRetorno=[2, 3, 4, 5, 6, 7, 8, 9, 10, 20,25, 30, 40, 50, 60, 70, 80, 90, 100,200,500,1000]\n",
    "    string_PR = [str(int) for int in PeriodoRetorno]\n",
    "    \n",
    "    print('Calculando TWL')\n",
    "\n",
    "    TWL_extrem_hist_tab = pd.DataFrame(index=table_point_carac.index,columns=string_PR+['MSLR','SLR_5','SLR_95'])\n",
    "    TWL_extrem_hist_tab.loc[:,['MSLR','SLR_5','SLR_95']] = 0\n",
    "    \n",
    "    Params_extrem_his    = pd.DataFrame(index=table_point_carac.index,columns=['c','loc','scale'])\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(table_point_carac))):\n",
    "\n",
    "        x_0 = table_point_carac.iloc[i,0]\n",
    "        y_0 = table_point_carac.iloc[i,1]\n",
    "\n",
    "        inProj = Proj(init='epsg:'+str(cod_epsg))\n",
    "        outProj = Proj(init='epsg:4326')\n",
    "\n",
    "        x,y = transform(inProj,outProj,x_0,y_0)\n",
    "\n",
    "\n",
    "        idoleaje = table_point_carac.iloc[i,2]\n",
    "        idmarea = table_point_carac.iloc[i,3]\n",
    "        idplaya = table_point_carac.iloc[i,3]\n",
    "        slope = table_point_carac.iloc[i,4]\n",
    "        CStock = table_point_carac.iloc[i,5]\n",
    "\n",
    "        point_selec_his = PtosObjtvo_ROW_his_nc.point.data[np.sqrt((x-PtosObjtvo_ROW_his_nc.lon.data)**2+(y-PtosObjtvo_ROW_his_nc.lat.data)**2).argmin()]\n",
    "\n",
    "        point_selec_MAT = PtosObjtvo_MAT_nc.point.data[np.sqrt((x-PtosObjtvo_MAT_nc.lon.data)**2+(y-PtosObjtvo_MAT_nc.lat.data)**2).argmin()]\n",
    "\n",
    "\n",
    "\n",
    "        Hs_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his,drop=True)['hs'].to_dataframe()\n",
    "        Hs_his.index = Hs_his.index.round('H')\n",
    "        try:\n",
    "            Hs_his[Hs_his.hs<0]=0.001\n",
    "        except:\n",
    "            Hs_his = Hs_his\n",
    "\n",
    "        Tp_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his,drop=True)['tps'].to_dataframe()\n",
    "        Tp_his.index = Tp_his.index.round('H')\n",
    "        try:\n",
    "            Tp_his[Tp_his.tps<=0]=np.mean(Tp_his[Tp_his>0],axis=0)\n",
    "        except:\n",
    "            Tp_his = Tp_his\n",
    "\n",
    "        SS_his = PtosObjtvo_ROW_his_nc.sel(point = point_selec_his,drop=True)['zeta'].to_dataframe()\n",
    "        SS_his.index = SS_his.index.round('H')\n",
    "\n",
    "\n",
    "        AT = PtosObjtvo_MAT_nc.sel(point = point_selec_MAT,drop=True)['tide'].to_dataframe()\n",
    "        AT.index = AT.index.round('H')\n",
    "\n",
    "        PMVE =AT.iloc[AT.values.argmax()-3:AT.values.argmax()+4]\n",
    "\n",
    "        setup_hist  = pd.DataFrame(index=Tp_his.index, columns=['setup'])\n",
    "\n",
    "        setup_hist.iloc[:,0]  = setup_fun (Tp_his.values,Hs_his.values*idoleaje,idplaya,slope,CStock)\n",
    "\n",
    "        ## Cálculo de la cota de inundación / TWL\n",
    "\n",
    "        Result_hist  = pd.concat((setup_hist,SS_his,AT),axis = 1).dropna()\n",
    "\n",
    "        TWL_hist  = Result_hist.iloc[:,0]+Result_hist.iloc[:,1]*idmarea+Result_hist.iloc[:,2]*idmarea+refNMMA\n",
    "        \n",
    "\n",
    "        TWL_hist  = Result_hist.iloc[:,0]+Result_hist.iloc[:,1]*idmarea+Result_hist.iloc[:,2]*idmarea+refNMMA\n",
    "           \n",
    "        TWL_extrem_his = Ajuste_GEV_KMA_Frechet(TWL_hist.resample('A').max().dropna().astype(float),path_project,save_plot=False,name_fig=None)[0]\n",
    "              \n",
    "        TWL_extrem_hist_tab.iloc[i,:len(PeriodoRetorno)]   = TWL_extrem_his.values.reshape(1,-1)\n",
    "        \n",
    "        Params_extrem_his.iloc[i,:]     = Ajuste_GEV_KMA_Frechet(TWL_hist.resample('A').max().dropna().astype(float),path_project,save_plot=True,name_fig='Params_extrem_'+'_ROW_'+'_POINT_'+str(i))[1:]\n",
    "              \n",
    "    TWL_extrem_hist_tab.to_csv(path_project+'sites/'+'TWL_extrem_ROW.csv')\n",
    "    Params_extrem_his.to_csv(path_project+'sites/'+'Params_extrem_ROW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tusrBCFlowLevel(table_points,file_TWL,temporal,path_project,TestDesc,Table_impact_zone,\n",
    "                     izcoast,BCSetID,BCTypeID_COAST,return_period,SLR,\n",
    "                    raw_q=True, point_X_river = None ,point_Y_river = None ,hidrograma=None):\n",
    "    \n",
    "    \"\"\"La siguiente función nos permite generar los ficheros de condición de contorno de RFSM.\n",
    "    \n",
    "       Parámetros:\n",
    "       ---------------------\n",
    "       table_points       : pandas dataframe. Dataframe con las coordenadas y características de los puntos donde asignaron las dinámicas\n",
    "       file_TWL           : pandas dataframe. Dataframe que contiene los puntos donde se extraen los períodos de retrono\n",
    "       temporal           : pandas dataframe. Dataframe que contiene el temporal de referencia.\n",
    "       path_project       : string. path donde se encuentra el proyecto de RFSM\n",
    "       TestDesc           : string. Nombre de la simulación\n",
    "       Table_impact_zone  : pandas dataframe.  Dataframe con el conjunto de impact zones\n",
    "       izcoast            : pandas dataframe.  Dataframe con el conjunto de impact zones donde se asigna condición de contorno costera\n",
    "       BCSetID            : int. Identificador de la simulación\n",
    "       BCTypeID_COAST     : int. Tipo de condición de contorno # 1 overtopping; # 2 level;\n",
    "       return_period      : int. Período de retorno estudiado\n",
    "       SLR                : float: Sea Level Rise\n",
    "       raw_q              : True or False: Si existe un punto donde se va introducir un caudal en un cauce poner en True\n",
    "       point_X_river      : float. Si existe un punto donde se va introducir un caudal en un cauce añadir \n",
    "       point_Y_river      : float. Si existe un punto donde se va introducir un caudal en un cauce poner en True\n",
    "       hidrograma         : array. Si existe un punto donde se va introducir un caudal en un cauce añadir hidrograma \n",
    "       \n",
    "        Salidas:\n",
    "        ---------------------\n",
    "        Fichero tusrBCFlowLevel necesario para la ejecución de RFSM.\n",
    "       \n",
    "       \n",
    "       \n",
    "        \"\"\"\n",
    "    \n",
    "    Table_impact_zone_edit =Table_impact_zone.copy()\n",
    "    Table_impact_zone_edit.loc[:,'BCTypeID'] = 0\n",
    "    \n",
    "    cI = temporal\n",
    "    cI = [cI[0]*1/10,cI[0]*3/10,cI[0]*5/10,cI[0]*7/10,cI,cI[-1]*7/10,cI[-1]*5/10,cI[-1]*3/10,cI[-1]*1/10]\n",
    "   \n",
    "    n_periods = len(cI)\n",
    "    \n",
    "    Table_impact_zone_edit.loc[izcoast.index,'BCTypeID'] = BCTypeID_COAST\n",
    "    \n",
    "    if raw_q==True:\n",
    "        \n",
    "        dist = np.sqrt((point_X_river-Table_impact_zone_edit.iloc[:,2])**2+(point_Y_river-Table_impact_zone_edit.iloc[:,3])**2)\n",
    "        point_select=dist.idxmin()\n",
    "        \n",
    "        Table_impact_zone_edit.loc[point_select,'BCTypeID'] = 10\n",
    "        \n",
    "    Results_TWL = pd.DataFrame(index=np.arange(0,sum(Table_impact_zone_edit.BCTypeID.values>0)*n_periods),columns=['BCSetID', 'BCTypeID', 'IZID', 'Time', 'BCValue'])\n",
    "    \n",
    "    Table_impact_zone_edit_2=Table_impact_zone_edit[Table_impact_zone_edit.BCTypeID!=0].copy()\n",
    "    \n",
    "    it=0\n",
    "    for iDZ in tqdm.tqdm(range(len(Table_impact_zone_edit_2))):\n",
    "        if (Table_impact_zone_edit_2['BCTypeID'].iloc[iDZ]==2) or (Table_impact_zone_edit_2['BCTypeID'].iloc[iDZ]==1):\n",
    "\n",
    "            x_pointIZID = Table_impact_zone_edit_2[' MidX'].values[iDZ]\n",
    "            y_pointIZID = Table_impact_zone_edit_2[' MidY'].values[iDZ]\n",
    "            IZID = Table_impact_zone_edit_2.index[iDZ]\n",
    "\n",
    "            dist = np.sqrt((x_pointIZID-table_points.iloc[:,0])**2+(y_pointIZID-table_points.iloc[:,1])**2)\n",
    "            point_select=dist.idxmin()\n",
    "\n",
    "            \n",
    "            TWL=table_points.loc[return_period,table_points.iloc[point_select]]\n",
    "            \n",
    "            cITr = cI/max(cI)*TWL\n",
    "            pos1= np.where(cITr == np.max(cITr))[0]\n",
    "\n",
    "            cITr[pos1]= cITr[pos1] + SLR\n",
    "            \n",
    "            if BCTypeID_COAST ==1:\n",
    "                cITr = cITr*izcoast.nCells[IZID]*cellsize\n",
    "            else:\n",
    "                cITr = cITr-izcoast.minH[IZID]\n",
    "                cITr[cITr<0]=0\n",
    "                \n",
    "            \n",
    "\n",
    "            Results_TWL.iloc[it:it+n_periods,0] = BCSetID\n",
    "            Results_TWL.iloc[it:it+n_periods,1] = BCTypeID_COAST\n",
    "            Results_TWL.iloc[it:it+n_periods,2] = IZID\n",
    "            Results_TWL.iloc[it:it+n_periods,3] = np.linspace(0,3600*len(cITr),len(cITr))\n",
    "            Results_TWL.iloc[it:it+n_periods,4] = cITr\n",
    "\n",
    "            it=it+n_periods\n",
    "\n",
    "            del W, MM, MA, Hs, setup, TWL\n",
    "\n",
    "        elif Table_impact_zone_edit_2['BCTypeID'].iloc[iDZ]==10:\n",
    "            print('Río')\n",
    "            Results_TWL.iloc[it:it+n_periods,0] = BCSetID\n",
    "            Results_TWL.iloc[it:it+n_periods,1] = 1\n",
    "            Results_TWL.iloc[it:it+n_periods,2] = IZID\n",
    "            Results_TWL.iloc[it:it+n_periods,3] = np.linspace(0,3600*len(hidrograma),len(hidrograma))\n",
    "            Results_TWL.iloc[it:it+n_periods,4] = hidrograma\n",
    "\n",
    "            it=it+n_periods\n",
    "            \n",
    "    Results_TWL.to_csv(path_project+'tests/'+TestDesc+'/Input_User/tusrBCFlowLevel.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcf_Dinamicas_DOW(path_files,path_output,model,scenario):\n",
    "    directory = list() \n",
    "    for file in os.listdir(path_files):\n",
    "        if file.startswith(\"Punto\"):\n",
    "            directory.append(file)\n",
    "    names_point = list()\n",
    "    lon = list()\n",
    "    lat = list()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for d, dd in enumerate(directory):\n",
    "        pos_ = [*find_all(dd, '[')]\n",
    "        pos_r = [*find_all(dd, ']')]\n",
    "        \n",
    "        names_point.append(dd[:pos_[0]-1])\n",
    "        lon.append(float(dd[pos_[0]+1:pos_r[0]]))\n",
    "        lat.append(float(dd[pos_[1]+1:pos_r[1]]))  \n",
    "        \n",
    "    for i,ii in enumerate(tqdm.tqdm(directory)):\n",
    "        if model == 'DOW2' and scenario == 'his':\n",
    "            path_file_ = path_files+directory[i]+'/'+directory[i]+'_'+'DOW2'+'.mat'\n",
    "        else:\n",
    "            path_file_ = path_files+directory[i]+'/'+directory[i]+'_'+model+'_'+scenario+'.mat'\n",
    "        \n",
    "        if i ==0:\n",
    "            mat    = scipy.io.loadmat(path_file_)\n",
    "            time   = mat['time'].flatten()\n",
    "            hs_o   = mat['hs'].reshape(1,-1)\n",
    "            tps_o  = mat['tps'].reshape(1,-1)\n",
    "            dir_o  = mat['dir'].reshape(1,-1)\n",
    "            zeta_o = mat['zeta'].reshape(1,-1)\n",
    "            tm02_o = mat['tm02'].reshape(1,-1)\n",
    "            bat_o = mat['bat']\n",
    "        else:\n",
    "            try:\n",
    "                mat = scipy.io.loadmat(path_file_)\n",
    "            except:\n",
    "                print('Error en el fichero: '+ path_file_)\n",
    "                continue\n",
    "            hs_o   = np.concatenate((hs_o, mat['hs'].reshape(1,-1)), axis=0)\n",
    "            tps_o  = np.concatenate((tps_o, mat['tps'].reshape(1,-1)), axis=0)\n",
    "            dir_o  = np.concatenate((dir_o, mat['dir'].reshape(1,-1)), axis=0)\n",
    "            zeta_o = np.concatenate((zeta_o, mat['zeta'].reshape(1,-1)), axis=0)\n",
    "            tm02_o = np.concatenate((tm02_o, mat['tm02'].reshape(1,-1)), axis=0)\n",
    "            bat_o = np.concatenate((bat_o, mat['bat']), axis=0)\n",
    "        \n",
    "    time_py = matDatenum2PYDatetime(time,unitTime = 'D')[0]\n",
    "    \n",
    "    nc = Dataset(path_output+'PtosObjtvo_DIN_'+model+'_'+scenario+'.nc', 'w', format='NETCDF4')\n",
    "    # Global Attributes \n",
    "    nc.description= 'Contiene las dinámicas del modelo '+model+' en el escenario '+scenario  \n",
    "    # nc dimensions\n",
    "    nc.createDimension('lon',  len(lon))\n",
    "    nc.createDimension('lat',  len(lat))\n",
    "    nc.createDimension('time',len(time))\n",
    "    nc.createDimension('point',len(names_point))\n",
    "    # crear variables\n",
    "    xx_nc=nc.createVariable('lon','float32', ('lon'))\n",
    "    yy_nc=nc.createVariable('lat','float32', ('lat'))\n",
    "    time_nc=nc.createVariable('time','float32',('time'))\n",
    "    point_nc = nc.createVariable('point','int',('point'))\n",
    "    hs_nc=nc.createVariable('hs','float32', ('time','point'))\n",
    "    tps_nc=nc.createVariable('tps','float32', ('time','point'))\n",
    "    dir_nc=nc.createVariable('dir','float32', ('time','point'))\n",
    "    zeta_nc=nc.createVariable('zeta','float32', ('time','point'))\n",
    "    tm02_nc=nc.createVariable('tm02','float32', ('time','point'))\n",
    "    bat_nc = nc.createVariable('bat','float32',('point'))\n",
    "    #units\n",
    "    xx_nc.units = 'degrees_east'\n",
    "    yy_nc.units = 'degrees_north'\n",
    "    time_nc.units='days since '+str(time_py[0].year)+'-01-01'\n",
    "    hs_nc.units='m'\n",
    "    tps_nc.units='s'\n",
    "    dir_nc.units='º'\n",
    "    zeta_nc.units='º'\n",
    "    tm02_nc.units='s'\n",
    "    point_nc.units = ''\n",
    "    bat_nc.units = 'm'\n",
    "    #long_name\n",
    "    xx_nc.long_name = 'longitude coordinate'\n",
    "    yy_nc.long_name = 'latitud coordinate'\n",
    "    time_nc.long_name='dias del año'\n",
    "    hs_nc.long_name='Altura de ola'\n",
    "    tps_nc.long_name='Período de pico suavizado'\n",
    "    dir_nc.long_name='Dirección'\n",
    "    zeta_nc.long_name='Marea meteorológica'\n",
    "    tm02_nc.long_name='Periodo medio'\n",
    "    point_nc.long_name = 'Puntos de estudio'\n",
    "    bat_nc.long_name = 'Profundidad'\n",
    "    # calendar\n",
    "    time_nc.calendar = 'standard'\n",
    "    # rellenar variables\n",
    "    point_nc[:]=np.arange(1,len(names_point)+1)\n",
    "    xx_nc[:]=lon\n",
    "    yy_nc[:]=lat\n",
    "    time_nc[:]=date2num(time_py.to_pydatetime(), units='days since '+str(time_py[0].year)+'-01-01', calendar='standard')\n",
    "    hs_nc[:]=hs_o[:,:].T\n",
    "    tps_nc[:]=tps_o[:,:].T\n",
    "    dir_nc[:]=dir_o[:,:].T\n",
    "    zeta_nc[:] = zeta_o[:,:].T\n",
    "    tm02_nc[:]=tm02_o[:,:].T\n",
    "    bat_nc[:]=bat_o[:].T\n",
    "    nc.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
